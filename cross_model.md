# Cross Model

|No.   |Figure   |Title   |Pub.  |Link|
|:----|:-----:|:-----:|:-----:|:---:|
|12|![multigrained](IM/multigrained.png)|__Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering__|__IJCAI 2020__|[`Paper`](https://www.ijcai.org/proceedings/2020/0153.pdf) [`Github`](https://github.com/astro-zihao/mucko)| 
|11|![multigrained](IM/multigrained.png)|__Multi-grained Attention with Object-level Grounding for Visual Question Answering__|__ACL 2019__|[`Paper`](https://aclanthology.org/P19-1349.pdf)| 
|10|![MUREL](IM/MUREL.png)|__MUREL: Multimodal Relational Reasoning for Visual Question Answering__|__CVPR 2019__|[`Paper`](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cadene_MUREL_Multimodal_Relational_Reasoning_for_Visual_Question_Answering_CVPR_2019_paper.pdf) [`Github`](https://github.com/Cadene/murel.bootstrap.pytorch)| 
|9|![mmgnn](IM/mmgnn.png)|__Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text__|__CVPR 2020__|[`Paper`](https://arxiv.org/abs/2003.13962) [`Github`](https://github.com/ricolike/mmgnn_textvqa)| 
|8|![wssis](IM/wssis.png)|__Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning__|__BMVC 2020__|[`Paper`](https://www.bmvc2020-conference.com/assets/papers/0430.pdf)| 
|8|![referit](IM/finegrained.png)|__Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning__|__CVPR 2020__|[`Paper`](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Fine-Grained_Video-Text_Retrieval_With_Hierarchical_Graph_Reasoning_CVPR_2020_paper.pdf) [`Github`](https://unclemedm.github.io/Refer-it-in-RGBD/)| 
|7|![referit](IM/referit.png)|__Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD Images__|__CVPR 2018__|[`Paper`](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Refer-It-in-RGBD_A_Bottom-Up_Approach_for_3D_Visual_Grounding_in_RGBD_CVPR_2021_paper.pdf) [`Github`](https://unclemedm.github.io/Refer-it-in-RGBD/)| 
|6|![accumulate](IM/accumulate.png)|__Visual Grounding via Accumulated Attention__|__CVPR 2018__|[`Paper`](https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf)|
|5|![Modulation](IM/Modulation.png)|__RGB-D Salient Object Detection with Cross-Modality Modulation and Selection__|__ECCV 2020__|[`Paper`](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530222.pdf) [`Github`](https://li-chongyi.github.io/Proj_ECCV20)|
|4|![depthsensitive](IM/depthsensitive.png)|__Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion__|__CVPR 2021__|[`Paper`](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Deep_RGB-D_Saliency_Detection_With_Depth-Sensitive_Attention_and_Automatic_Multi-Modal_CVPR_2021_paper.pdf) [`Github`](https://github.com/sunpeng1996/DSA2F) |
|3|![attngan](IM/RIS.png)|__Cross-Modal Self-Attention Network for Referring Image Segmentation__|__CVPR 2019__|[`Paper`](http://openaccess.thecvf.com/content_CVPR_2019/papers/Ye_Cross-Modal_Self-Attention_Network_for_Referring_Image_Segmentation_CVPR_2019_paper.pdf) [`Github`](https://github.com/NanWangAC/CMSA-Net) |
|2|![stackgan](IM/RE1.png)|__Cross-Modal Relationship Inference for Grounding Referring Expressions__|__CVPR 2019__|[`Paper`](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Cross-Modal_Relationship_Inference_for_Grounding_Referring_Expressions_CVPR_2019_paper.pdf) |
|1|![dcgan_network](IM/RR.png)|__Language-Conditioned Graph Networks for Relational Reasoning__|__ICCV 2019__|[`Paper`](https://arxiv.org/pdf/1905.04405.pdf) [`Github`](https://github.com/ronghanghu/lcgn) |

